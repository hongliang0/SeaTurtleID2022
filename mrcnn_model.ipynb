{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c521170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.30s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.67s)\n",
      "creating index...\n",
      "index created!\n",
      "Starting epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5: 100%|███████████████████| 999/999 [1:53:55<00:00,  6.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed with loss: 0.5177559852600098\n",
      "Starting epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/5: 100%|███████████████████| 999/999 [2:52:08<00:00, 10.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed with loss: 0.33852246403694153\n",
      "Starting epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/5: 100%|███████████████████| 999/999 [1:39:40<00:00,  5.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed with loss: 0.20086316764354706\n",
      "Starting epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/5: 100%|███████████████████| 999/999 [1:57:04<00:00,  7.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed with loss: 0.280353307723999\n",
      "Starting epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/5: 100%|███████████████████| 999/999 [5:09:19<00:00, 18.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed with loss: 0.3652145266532898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Custom Dataset class for SeaTurtleID2022\n",
    "class SeaTurtleDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # Load image\n",
    "        img_info = self.coco.loadImgs([img_id])[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load masks\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "            # Convert bbox format from [xmin, ymin, width, height] to [xmin, ymin, xmax, ymax]\n",
    "            xmin, ymin, width, height = ann['bbox']\n",
    "            xmax = xmin + width\n",
    "            ymax = ymin + height\n",
    "            boxes.append([xmin, ymin, xmax, ymax])  # Converted format\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert data to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        # Compute area and handle target dict\n",
    "        area = torch.as_tensor([ann['area'] for ann in anns], dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor([ann.get('iscrowd', 0) for ann in anns], dtype=torch.int64)  # Ensure 'iscrowd' key exists\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Split dataset into train and val sets using metadata_splits.csv\n",
    "def split_dataset(metadata_file, train_size=1000, val_size=200):\n",
    "    splits = pd.read_csv(metadata_file)\n",
    "    \n",
    "    # Use the 'split_open' column for splitting\n",
    "    train_ids = splits[splits['split_open'] == 'train']['id'].tolist()\n",
    "    val_ids = splits[splits['split_open'] == 'val']['id'].tolist()\n",
    "\n",
    "    # Randomly sample specified number of training and validation samples\n",
    "    train_ids = random.sample(train_ids, min(train_size, len(train_ids)))\n",
    "    val_ids = random.sample(val_ids, min(val_size, len(val_ids)))\n",
    "\n",
    "    return train_ids, val_ids\n",
    "\n",
    "# Create data loaders\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def get_data_loaders(img_dir, ann_file, metadata_file, batch_size=2):\n",
    "    train_ids, val_ids = split_dataset(metadata_file, train_size=1000, val_size=200)\n",
    "\n",
    "    # Create datasets\n",
    "    transforms = T.Compose([\n",
    "        T.ToTensor(),  # Converts image to tensor and normalizes to [0, 1]\n",
    "    ])\n",
    "\n",
    "    train_dataset = SeaTurtleDataset(img_dir, ann_file, transforms=transforms)\n",
    "    val_dataset = SeaTurtleDataset(img_dir, ann_file, transforms=transforms)\n",
    "\n",
    "    # Filter datasets based on sampled IDs\n",
    "    train_dataset.image_ids = [img_id for img_id in train_dataset.image_ids if img_id in train_ids]\n",
    "    val_dataset.image_ids = [img_id for img_id in val_dataset.image_ids if img_id in val_ids]\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Example usage\n",
    "img_dir = \"./turtles-data/data\"\n",
    "ann_file = \"./turtles-data/data/updated_annotations.json\"\n",
    "metadata_file = \"./turtles-data/data/metadata_splits.csv\"\n",
    "batch_size = 1\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(img_dir, ann_file, metadata_file, batch_size=batch_size)\n",
    "\n",
    "# Load the Mask R-CNN model\n",
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# Load pre-trained model with recommended weights\n",
    "weights = MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "num_classes = 4  # 3 classes (turtle, flipper, head) + background\n",
    "\n",
    "# Get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Get number of input features for the mask classifier\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "\n",
    "# Replace the mask predictor with a new one\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "# Training loop example\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "def visualize_prediction(image, predicted_masks, true_masks, epoch):\n",
    "    \"\"\"\n",
    "    Visualizes predictions and ground truth masks on a single image.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Convert image tensor to PIL image for visualization\n",
    "    image = F.to_pil_image(image.cpu())\n",
    "\n",
    "    # Plot input image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title(\"Input Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    # Plot predicted mask\n",
    "    predicted_mask = predicted_masks.sum(dim=0).cpu().numpy() > 0  # Aggregate masks for visualization\n",
    "    ax[1].imshow(image)\n",
    "    ax[1].imshow(predicted_mask, alpha=0.5, cmap='jet')\n",
    "    ax[1].set_title(f\"Predicted Masks - Epoch {epoch+1}\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    # Plot true mask\n",
    "    true_mask = true_masks.sum(dim=0).cpu().numpy() > 0\n",
    "    ax[2].imshow(image)\n",
    "    ax[2].imshow(true_mask, alpha=0.5, cmap='jet')\n",
    "    ax[2].set_title(\"True Masks\")\n",
    "    ax[2].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def train_model(train_loader, val_loader, model, optimizer, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for images, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass and loss calculation\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed with loss: {losses.item()}\")\n",
    "\n",
    "        # Validation step: Show predictions for a few images\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_targets in val_loader:\n",
    "                val_images = [img.to(device) for img in val_images]\n",
    "                val_targets = [{k: v.to(device) for k, v in t.items()} for t in val_targets]\n",
    "                outputs = model(val_images)  # Get model predictions\n",
    "\n",
    "                # Visualize the first validation image's predictions vs ground truth\n",
    "                for i, (val_image, output) in enumerate(zip(val_images, outputs)):\n",
    "                    predicted_masks = output['masks'] > 0.5  # Threshold to get binary masks\n",
    "                    true_masks = val_targets[i]['masks']\n",
    "                    visualize_prediction(val_image, predicted_masks, true_masks, epoch)\n",
    "\n",
    "                    break  # Only visualize one sample per epoch\n",
    "                break  # Only show one batch per epoch\n",
    "\n",
    "# Call the training function\n",
    "train_model(train_loader, val_loader, model, optimizer, device, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4259134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.08s)\n",
      "creating index...\n",
      "index created!\n",
      "Annotation 0: bbox = [644.0, 441.0, 70.0, 78.0]\n",
      "Annotation 1: bbox = [913.0, 582.0, 128.0, 184.0]\n",
      "Annotation 2: bbox = [1137.0, 478.0, 167.0, 48.0]\n",
      "Annotation 3: bbox = [660.0, 569.0, 50.0, 216.0]\n",
      "Annotation 4: bbox = [646.0, 374.0, 655.0, 406.0]\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "resize_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "annotation_file = \"./turtles-data/data/updated_annotations.json\"\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "class SeaTurtleDataset( Dataset ):\n",
    "    def __init__ (self, image_ids, transform = None):\n",
    "        self.coco = COCO('./turtles-data/data/updated_annotations.json')\n",
    "        self.image_ids = image_ids\n",
    "        self.cat_ids = self.coco.getCatIds()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__( self ):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image_data = self.coco.loadImgs([image_id])[0]\n",
    "\n",
    "        image_path = os.path.join('./turtles-data/data', image_data['file_name'])\n",
    "        image = cv.imread(image_path)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = self._getmask(self.image_ids[index], image)\n",
    "\n",
    "\n",
    "\n",
    "        if self.transform is  not  None : \n",
    "            augmentations = self.transform(image=image, mask =mask) \n",
    "            image = augmentations[ 'image' ] \n",
    "            mask = augmentations[ 'mask' ] \n",
    "\n",
    "        return image, mask \n",
    "    \n",
    "    def _getmask(self, image_id, image):\n",
    "        '''\n",
    "        Background (0):   000000000000000\n",
    "        Turtle Body (1):  111111100000000\n",
    "        Flippers (2):     112222200000000\n",
    "        Head (3):         112222233300000\n",
    "\n",
    "        Final Mask:       112222233300000\n",
    "        '''\n",
    "        categories = {\n",
    "            'turtle': 1,\n",
    "            'flipper': 2,\n",
    "            'head': 3\n",
    "        }\n",
    "        # Initialize the final mask with zeros\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        # Process each category\n",
    "        for category_name, category_id in categories.items():\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id, catIds=category_id, iscrowd=None)\n",
    "            annotations = self.coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # Create a temporary mask for the current category\n",
    "            temp_mask = np.zeros_like(mask)\n",
    "            \n",
    "            for ann in annotations:\n",
    "                temp_mask += self.coco.annToMask(ann)\n",
    "            # Assign category-specific value to the final mask\n",
    "            if category_name == 'turtle':\n",
    "                mask[temp_mask > 0] = 1\n",
    "            elif category_name == 'flipper':\n",
    "                mask[temp_mask > 0] = 2\n",
    "            elif category_name == 'head':\n",
    "                mask[temp_mask > 0] = 3\n",
    "\n",
    "        return mask\n",
    "    \n",
    "train_ids, test_ids = train_test_split(coco.getImgIds()[:4000], test_size=0.1, random_state=42)\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = SeaTurtleDataset(train_ids, transform=resize_transform)\n",
    "val_dataset = SeaTurtleDataset(val_ids, transform=resize_transform)\n",
    "test_dataset = SeaTurtleDataset(test_ids, transform=resize_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_deeplab_model(num_classes):\n",
    "    model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "    model.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, num_classes)\n",
    "    return model\n",
    "\n",
    "def compute_IoU(outputs, masks, target_class):\n",
    "    # Get the predicted class for each pixel\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "    intersection = ((outputs == target_class) & (masks == target_class)).sum().item()\n",
    "    union = ((outputs == target_class) | (masks == target_class)).sum().item()\n",
    "    if union == 0:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return intersection / union\n",
    "    \n",
    "num_classes = 4  # background + 3 classes\n",
    "model = get_deeplab_model(num_classes)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_prediction(image, mask, prediction, figsize=(15,5)):\n",
    "   # Convert tensors to numpy arrays\n",
    "   if isinstance(image, torch.Tensor):\n",
    "       image = image.cpu().numpy()\n",
    "       if image.shape[0] == 3:\n",
    "           image = image.transpose(1, 2, 0)\n",
    "       \n",
    "   if isinstance(mask, torch.Tensor):\n",
    "       mask = mask.cpu().numpy()\n",
    "       \n",
    "   if isinstance(prediction, torch.Tensor):\n",
    "       prediction = prediction.cpu().numpy()\n",
    "   \n",
    "   # Denormalize image\n",
    "   mean = np.array([0.485, 0.456, 0.406])\n",
    "   std = np.array([0.229, 0.224, 0.225])\n",
    "   image = np.clip((image * std + mean), 0, 1)\n",
    "   \n",
    "   # Create color maps for mask and prediction\n",
    "   colors = [(0,0,0), (1,0,0), (0,1,0), (0,0,1)]\n",
    "   colored_mask = np.zeros((*mask.shape, 3))\n",
    "   colored_pred = np.zeros((*prediction.shape, 3))\n",
    "   \n",
    "   for i, color in enumerate(colors):\n",
    "       colored_mask[mask == i] = color\n",
    "       colored_pred[prediction == i] = color\n",
    "   \n",
    "   # Plot\n",
    "   plt.figure(figsize=figsize)\n",
    "   \n",
    "   plt.subplot(1, 3, 1)\n",
    "   plt.imshow(image)\n",
    "   plt.title('Original Image')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.subplot(1, 3, 2)\n",
    "   plt.imshow(colored_mask)\n",
    "   plt.title('Ground Truth Mask')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.subplot(1, 3, 3)\n",
    "   plt.imshow(colored_pred)\n",
    "   plt.title('Model Prediction')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "\n",
    "\n",
    "model = get_deeplab_model(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define class labels for each category\n",
    "turtle_class = 1\n",
    "flipper_class = 2\n",
    "head_class = 3\n",
    "\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Print every 5 batches\n",
    "        if batch_idx % 5 == 0:  \n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device).long()\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss after Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Compute mIoU on the test set\n",
    "    turtle_IoUs, flipper_IoUs, head_IoUs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device).long()\n",
    "            outputs = model(images)['out']\n",
    "            \n",
    "            # Compute IoU for each category\n",
    "            for i in range(len(images)):  # Process each image in the batch\n",
    "                turtle_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], turtle_class))\n",
    "                flipper_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], flipper_class))\n",
    "                head_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], head_class))\n",
    "\n",
    "    turtle_mIoU = np.nanmean(turtle_IoUs)\n",
    "    flipper_mIoU = np.nanmean(flipper_IoUs)\n",
    "    head_mIoU = np.nanmean(head_IoUs)\n",
    "    \n",
    "    print(f\"Turtle (Carapace) mIoU on Test Set after Epoch {epoch+1}: {turtle_mIoU:.4f}\")\n",
    "    print(f\"Flippers mIoU on Test Set after Epoch {epoch+1}: {flipper_mIoU:.4f}\")\n",
    "    print(f\"Head mIoU on Test Set after Epoch {epoch+1}: {head_mIoU:.4f}\")\n",
    "    \n",
    "    # Visualize last few predictions after each epoch\n",
    "    with torch.no_grad():\n",
    "        images, masks = [], []\n",
    "        for batch_images, batch_masks in train_loader:\n",
    "            images, masks = batch_images[-3:], batch_masks[-3:]\n",
    "        \n",
    "        images = images.to(device)\n",
    "        outputs = model(images)['out']\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        print(f\"\\nPredictions after Epoch {epoch+1}:\")\n",
    "        for i in range(min(3, len(images))):\n",
    "            visualize_prediction(\n",
    "                images[i],\n",
    "                masks[i],\n",
    "                predictions[i]\n",
    "            )\n",
    "    \n",
    "    model.train()  # Set back to training mode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12962a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
