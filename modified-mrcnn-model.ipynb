{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a536b542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.75s)\n",
      "creating index...\n",
      "index created!\n",
      "----------------- Dataset Split -----------------\n",
      "\n",
      "Training set size: 5293\n",
      "Validation set size: 1117\n",
      "Test set size: 2299\n",
      "\n",
      "loading annotations into memory...\n",
      "Done (t=1.64s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=7.34s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.51s)\n",
      "creating index...\n",
      "index created!\n",
      "----------------- Data Loaders -----------------\n",
      "\n",
      "Training loader: 2647 batches\n",
      "Validation loader: 559 batches\n",
      "Test loader: 1150 batches\n",
      "\n",
      "Starting epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5:   0%|                   | 11/2647 [02:09<8:37:55, 11.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 320\u001b[0m\n\u001b[1;32m    317\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m train_model(train_loader, val_loader, model, optimizer, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Load the best model and perform testing\u001b[39;00m\n\u001b[1;32m    323\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_maskrcnn_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[12], line 246\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    245\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 246\u001b[0m     losses\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    247\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    249\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# put this file to the archive file\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Define the SeaTurtleDataset class\n",
    "class SeaTurtleDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # Load the image\n",
    "        img_info = self.coco.loadImgs([img_id])[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Load masks and other annotations\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            mask = self.coco.annToMask(ann)\n",
    "            masks.append(mask)\n",
    "            xmin, ymin, width, height = ann['bbox']\n",
    "            xmax = xmin + width\n",
    "            ymax = ymin + height\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        area = torch.as_tensor([ann['area'] for ann in anns], dtype=torch.float32)\n",
    "        iscrowd = torch.as_tensor([ann.get('iscrowd', 0) for ann in anns], dtype=torch.int64)\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks,\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            # Apply the transformations to image and target\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Define the collate_fn function\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Define the function to calculate IOU\n",
    "def calculate_iou(pred_mask, true_mask):\n",
    "    pred_mask = pred_mask.cpu().numpy().astype(bool)\n",
    "    true_mask = true_mask.cpu().numpy().astype(bool)\n",
    "\n",
    "    intersection = np.logical_and(pred_mask, true_mask)\n",
    "    union = np.logical_or(pred_mask, true_mask)\n",
    "    iou_score = np.sum(intersection) / np.sum(union) if np.sum(union) > 0 else 0.0\n",
    "    return iou_score\n",
    "\n",
    "# Define the visualization function\n",
    "def visualize_prediction(image, predicted_masks, true_masks, epoch, iou_score=None):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Convert image for visualization\n",
    "    image = F.to_pil_image(image.cpu())\n",
    "\n",
    "    # Display the original image\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    # Display the predicted masks\n",
    "    predicted_mask = predicted_masks.sum(dim=0).cpu().numpy() > 0\n",
    "    ax[1].imshow(image)\n",
    "    ax[1].imshow(predicted_mask, alpha=0.5, cmap='jet')\n",
    "    title_pred = f\"Predicted Masks - Epoch {epoch+1}\"\n",
    "    if iou_score is not None:\n",
    "        title_pred += f\"\\nIOU: {iou_score:.4f}\"\n",
    "    ax[1].set_title(title_pred)\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    # Display the true masks\n",
    "    true_mask = true_masks.sum(dim=0).cpu().numpy() > 0\n",
    "    ax[2].imshow(image)\n",
    "    ax[2].imshow(true_mask, alpha=0.5, cmap='jet')\n",
    "    ax[2].set_title(\"True Masks\")\n",
    "    ax[2].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Define transformations with data augmentation\n",
    "class ComposeTransform:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            image = F.hflip(image)\n",
    "            width = image.width\n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes[:, [0, 2]] = width - boxes[:, [2, 0]]\n",
    "            target[\"boxes\"] = boxes\n",
    "            if \"masks\" in target:\n",
    "                target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        return image, target\n",
    "\n",
    "class RandomVerticalFlip:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            image = F.vflip(image)\n",
    "            height = image.height\n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes[:, [1, 3]] = height - boxes[:, [3, 1]]\n",
    "            target[\"boxes\"] = boxes\n",
    "            if \"masks\" in target:\n",
    "                target[\"masks\"] = target[\"masks\"].flip(-2)\n",
    "        return image, target\n",
    "\n",
    "class ColorJitterTransform:\n",
    "    def __init__(self, brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1):\n",
    "        self.color_jitter = T.ColorJitter(brightness, contrast, saturation, hue)\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = self.color_jitter(image)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "# Function to get data loaders\n",
    "def get_data_loaders(img_dir, ann_file, metadata_file, batch_size=2):\n",
    "    # Load metadata\n",
    "    metadata = pd.read_csv(metadata_file)\n",
    "    \n",
    "    # Define data transformations with data augmentation for training\n",
    "    train_transforms = ComposeTransform([\n",
    "        ColorJitterTransform(),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomVerticalFlip(),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # For validation and testing, we use only ToTensor\n",
    "    val_test_transforms = ComposeTransform([\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "    full_dataset = SeaTurtleDataset(img_dir, ann_file, transforms=None)\n",
    "    \n",
    "    # Create mappings between image IDs and filenames\n",
    "    img_to_filename = {img_id: full_dataset.coco.loadImgs(img_id)[0][\"file_name\"] for img_id in full_dataset.image_ids}\n",
    "    file_to_img = {v: k for k, v in img_to_filename.items()}\n",
    "    \n",
    "    # Split the dataset based on the 'split_open' column in metadata\n",
    "    train_img_ids = [file_to_img[filename] for filename in metadata[metadata[\"split_open\"] == \"train\"][\"file_name\"] if filename in file_to_img]\n",
    "    val_img_ids = [file_to_img[filename] for filename in metadata[metadata[\"split_open\"] == \"valid\"][\"file_name\"] if filename in file_to_img]\n",
    "    test_img_ids = [file_to_img[filename] for filename in metadata[metadata[\"split_open\"] == \"test\"][\"file_name\"] if filename in file_to_img]\n",
    "    \n",
    "    # Verify the size of each split\n",
    "    train_size, val_size, test_size = len(train_img_ids), len(val_img_ids), len(test_img_ids)\n",
    "    print(f\"----------------- Dataset Split -----------------\\n\")\n",
    "    print(f\"Training set size: {train_size}\")\n",
    "    print(f\"Validation set size: {val_size}\")\n",
    "    print(f\"Test set size: {test_size}\\n\")\n",
    "    \n",
    "    # Create subsets for each split\n",
    "    train_indices = [full_dataset.image_ids.index(img_id) for img_id in train_img_ids]\n",
    "    val_indices = [full_dataset.image_ids.index(img_id) for img_id in val_img_ids]\n",
    "    test_indices = [full_dataset.image_ids.index(img_id) for img_id in test_img_ids]\n",
    "    \n",
    "    # Assign transforms to datasets\n",
    "    train_dataset = Subset(SeaTurtleDataset(img_dir, ann_file, transforms=train_transforms), train_indices)\n",
    "    val_dataset = Subset(SeaTurtleDataset(img_dir, ann_file, transforms=val_test_transforms), val_indices)\n",
    "    test_dataset = Subset(SeaTurtleDataset(img_dir, ann_file, transforms=val_test_transforms), test_indices)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    print(f\"----------------- Data Loaders -----------------\\n\")\n",
    "    print(f\"Training loader: {len(train_loader)} batches\")\n",
    "    print(f\"Validation loader: {len(val_loader)} batches\")\n",
    "    print(f\"Test loader: {len(test_loader)} batches\\n\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Function to train the model and save the best model\n",
    "def train_model(train_loader, val_loader, model, optimizer, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    best_iou = 0.0  # Initialize the best IOU\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        print(f\"Starting epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass and loss computation\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            epoch_loss += losses.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} average training loss: {avg_loss}\")\n",
    "\n",
    "        # Validation step: calculate average IOU on validation set\n",
    "        model.eval()\n",
    "        val_iou_scores = []\n",
    "        with torch.no_grad():\n",
    "            for val_images, val_targets in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\"):\n",
    "                val_images = [img.to(device) for img in val_images]\n",
    "                val_targets = [{k: v.to(device) for k, v in t.items()} for t in val_targets]\n",
    "                outputs = model(val_images)\n",
    "\n",
    "                for i, (val_image, output) in enumerate(zip(val_images, outputs)):\n",
    "                    predicted_masks = output['masks'] > 0.5\n",
    "                    true_masks = val_targets[i]['masks']\n",
    "\n",
    "                    if predicted_masks.shape[0] > 0 and true_masks.shape[0] > 0:\n",
    "                        pred_mask_combined = predicted_masks.sum(dim=0)\n",
    "                        true_mask_combined = true_masks.sum(dim=0)\n",
    "                        iou_score = calculate_iou(pred_mask_combined, true_mask_combined)\n",
    "                        val_iou_scores.append(iou_score)\n",
    "                    else:\n",
    "                        val_iou_scores.append(0.0)\n",
    "\n",
    "        average_val_iou = sum(val_iou_scores) / len(val_iou_scores)\n",
    "        print(f\"Epoch {epoch+1} average validation IOU: {average_val_iou:.4f}\")\n",
    "\n",
    "        # Check if this is the best model\n",
    "        if average_val_iou > best_iou:\n",
    "            best_iou = average_val_iou\n",
    "            best_model_path = \"best_maskrcnn_model.pth\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with IOU: {best_iou:.4f}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths and parameters\n",
    "    img_dir = \"./turtles-data/data\"\n",
    "    ann_file = \"./turtles-data/data/updated_annotations.json\"\n",
    "    metadata_file = \"./turtles-data/data/metadata_splits.csv\"\n",
    "    batch_size = 2\n",
    "\n",
    "    # Get data loaders\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(img_dir, ann_file, metadata_file, batch_size=batch_size)\n",
    "\n",
    "    # Load the pre-trained Mask R-CNN model\n",
    "    import torchvision\n",
    "    from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "    from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "    weights = MaskRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "    model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "    num_classes = 4  # 3 classes + background\n",
    "\n",
    "    # Replace the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace the mask predictor\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "\n",
    "    # Set device and optimizer\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # Start training\n",
    "    train_model(train_loader, val_loader, model, optimizer, device, num_epochs=5)\n",
    "\n",
    "    # Load the best model and perform testing\n",
    "    model.load_state_dict(torch.load('best_maskrcnn_model.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Perform inference on the test set and calculate IOU\n",
    "    iou_scores = []\n",
    "    with torch.no_grad():\n",
    "        for test_images, test_targets in tqdm(test_loader, desc=\"Testing\"):\n",
    "            test_images = [img.to(device) for img in test_images]\n",
    "            test_targets = [{k: v.to(device) for k, v in t.items()} for t in test_targets]\n",
    "            outputs = model(test_images)\n",
    "            \n",
    "            for i, (test_image, output) in enumerate(zip(test_images, outputs)):\n",
    "                predicted_masks = output['masks'] > 0.5\n",
    "                true_masks = test_targets[i]['masks']\n",
    "                \n",
    "                if predicted_masks.shape[0] > 0 and true_masks.shape[0] > 0:\n",
    "                    pred_mask_combined = predicted_masks.sum(dim=0)\n",
    "                    true_mask_combined = true_masks.sum(dim=0)\n",
    "                    iou_score = calculate_iou(pred_mask_combined, true_mask_combined)\n",
    "                    iou_scores.append(iou_score)\n",
    "                else:\n",
    "                    iou_scores.append(0.0)\n",
    "\n",
    "                # Visualize prediction results (optional)\n",
    "                # visualize_prediction(test_image, predicted_masks, true_masks, epoch=5, iou_score=iou_score)\n",
    "\n",
    "    # Calculate average IOU\n",
    "    average_iou = sum(iou_scores) / len(iou_scores)\n",
    "    print(f\"\\nAverage IOU on Test Set: {average_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbafef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
