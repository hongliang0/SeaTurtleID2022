{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9517 Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 : DeepLabV3+\n",
    "\n",
    "Please run this file after dataVisualisation.ipynb.\n",
    "\n",
    "Please run all the cells in this file first and follow the instructions in this file. This notebook's purpose is to perform DeepLabV3+ model on the turtle dataset, we will also be loading in our data from /models/dataset.pth which was created in dataVisualisation.ipyb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load back in all the datasplits we created back in dataVisualisation.ipynb. To do this, we will need to again import back in the SeaTurtleDataset custom class we created for this dataset. This simply involves copying what we had back to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# \n",
    "# Nothing new here, just same as dataVisualisation.ipynb.\n",
    "# \n",
    "###################################################################\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "resize_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(512, 512, p=1),  # Resize to 512 x 512\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        ),  # Normalises pixel values\n",
    "        ToTensorV2(),  # Converts to PyTorch tensor\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category_ids\"]),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class SeaTurtleDataset(Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None, target_size=(512, 512)):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annotation)\n",
    "        self.transforms = transforms\n",
    "        self.cat_ids = self.coco.getCatIds()\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Image params\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root, img_info[\"file_name\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # BBox and category IDs\n",
    "        bbox = [ann[\"bbox\"] for ann in anns]\n",
    "        category_ids = [ann[\"category_id\"] for ann in anns]\n",
    "\n",
    "        # Apply transformations\n",
    "        original_size = img.size\n",
    "        image = np.array(img)\n",
    "\n",
    "        # Masks from annotations\n",
    "        masks = self._getmask(self.img_ids[idx], image)\n",
    "\n",
    "        if self.transforms and original_size != self.target_size:\n",
    "            transformed = self.transforms(\n",
    "                image=image, bboxes=bbox, masks=[masks], category_ids=category_ids\n",
    "            )\n",
    "            image = transformed[\"image\"]  # Resize image\n",
    "            bbox = transformed[\"bboxes\"]  # Resize bounding boxes\n",
    "            masks = transformed[\"masks\"][0]  # Resize masks\n",
    "            category_ids = transformed[\"category_ids\"]\n",
    "        else:\n",
    "            image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1)\n",
    "            masks = torch.tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        # Convert bounding boxes and category IDs to tensors\n",
    "        bboxes = torch.tensor(bbox, dtype=torch.float32)\n",
    "        category_ids = torch.tensor(category_ids, dtype=torch.int64)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"bboxes\": bboxes,\n",
    "            \"masks\": masks,\n",
    "            \"category_ids\": category_ids,\n",
    "            \"original_size\": original_size,\n",
    "            \"target_size\": self.target_size,\n",
    "        }\n",
    "\n",
    "    def _getmask(self, image_id, image):\n",
    "        categories = {\"turtle\": 1, \"flipper\": 2, \"head\": 3}\n",
    "        # Initialise the mask\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        # Process each category\n",
    "        for category_name, category_id in categories.items():\n",
    "            ann_ids = self.coco.getAnnIds(\n",
    "                imgIds=image_id, catIds=category_id, iscrowd=None\n",
    "            )\n",
    "            annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Create a temporary mask for the current category\n",
    "            temp_mask = np.zeros_like(mask)\n",
    "\n",
    "            for ann in annotations:\n",
    "                temp_mask += self.coco.annToMask(ann)\n",
    "            # Assign category-specific value to the final mask\n",
    "            if category_name == \"turtle\":\n",
    "                mask[temp_mask > 0] = 1\n",
    "            elif category_name == \"flipper\":\n",
    "                mask[temp_mask > 0] = 2\n",
    "            elif category_name == \"head\":\n",
    "                mask[temp_mask > 0] = 3\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load our train val split from /models into our code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load in our presplitted data from dataVisualisation.ipynb\n",
    "loaded_dataset = torch.load('models/dataset.pth')\n",
    "\n",
    "# Split as we need\n",
    "train_dataset = loaded_dataset['train']\n",
    "val_dataset = loaded_dataset['val']\n",
    "test_dataset = loaded_dataset['test']\n",
    "\n",
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IoU(outputs, masks, target_class):\n",
    "    # Get the predicted class for each pixel\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "    intersection = ((outputs == target_class) & (masks == target_class)).sum().item()\n",
    "    union = ((outputs == target_class) | (masks == target_class)).sum().item()\n",
    "    if union == 0:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return intersection / union\n",
    "    \n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet101\",\n",
    "    encoder_weights=\"imagenet\",  \n",
    "    classes=4,                    \n",
    "    activation='softmax2d'\n",
    ")\n",
    "\n",
    "num_classes = 4  # background + 3 classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt generated function\n",
    "def visualize_prediction(image, mask, prediction, figsize=(15,5)):\n",
    "   # Convert tensors to numpy arrays\n",
    "   if isinstance(image, torch.Tensor):\n",
    "       image = image.cpu().numpy()\n",
    "       if image.shape[0] == 3:\n",
    "           image = image.transpose(1, 2, 0)\n",
    "       \n",
    "   if isinstance(mask, torch.Tensor):\n",
    "       mask = mask.cpu().numpy()\n",
    "       \n",
    "   if isinstance(prediction, torch.Tensor):\n",
    "       prediction = prediction.cpu().numpy()\n",
    "   \n",
    "   # Denormalize image\n",
    "   mean = np.array([0.485, 0.456, 0.406])\n",
    "   std = np.array([0.229, 0.224, 0.225])\n",
    "   image = np.clip((image * std + mean), 0, 1)\n",
    "   \n",
    "   # Create color maps for mask and prediction\n",
    "   colors = [(0,0,0), (1,0,0), (0,1,0), (0,0,1)]\n",
    "   colored_mask = np.zeros((*mask.shape, 3))\n",
    "   colored_pred = np.zeros((*prediction.shape, 3))\n",
    "   \n",
    "   for i, color in enumerate(colors):\n",
    "       colored_mask[mask == i] = color\n",
    "       colored_pred[prediction == i] = color\n",
    "   \n",
    "   # Plot\n",
    "   plt.figure(figsize=figsize)\n",
    "   \n",
    "   plt.subplot(1, 3, 1)\n",
    "   plt.imshow(image)\n",
    "   plt.title('Original Image')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.subplot(1, 3, 2)\n",
    "   plt.imshow(colored_mask)\n",
    "   plt.title('Ground Truth Mask')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.subplot(1, 3, 3)\n",
    "   plt.imshow(colored_pred)\n",
    "   plt.title('Model Prediction')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Define class labels for each category\n",
    "turtle_class = 1\n",
    "flipper_class = 2\n",
    "head_class = 3\n",
    "\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Print every 5 batches\n",
    "        if batch_idx % 5 == 0:  \n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device).long()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss after Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Compute mIoU on the test set\n",
    "    turtle_IoUs, flipper_IoUs, head_IoUs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device).long()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute IoU for each category\n",
    "            for i in range(len(images)):  # Process each image in the batch\n",
    "                turtle_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], turtle_class))\n",
    "                flipper_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], flipper_class))\n",
    "                head_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], head_class))\n",
    "\n",
    "    turtle_mIoU = np.nanmean(turtle_IoUs)\n",
    "    flipper_mIoU = np.nanmean(flipper_IoUs)\n",
    "    head_mIoU = np.nanmean(head_IoUs)\n",
    "    \n",
    "    print(f\"Turtle (Carapace) mIoU on Test Set after Epoch {epoch+1}: {turtle_mIoU:.4f}\")\n",
    "    print(f\"Flippers mIoU on Test Set after Epoch {epoch+1}: {flipper_mIoU:.4f}\")\n",
    "    print(f\"Head mIoU on Test Set after Epoch {epoch+1}: {head_mIoU:.4f}\")\n",
    "    \n",
    "    # Visualize last few predictions after each epoch\n",
    "    with torch.no_grad():\n",
    "        # Get a single batch from the train_loader\n",
    "        batch_images, batch_masks = next(iter(train_loader))\n",
    "        \n",
    "        # Select the last 3 images in the batch\n",
    "        images = batch_images[-3:].to(device)\n",
    "        masks = batch_masks[-3:]\n",
    "        \n",
    "        # Generate model predictions\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Visualize predictions\n",
    "        print(f\"\\nPredictions after Epoch {epoch+1}:\")\n",
    "        for i in range(3):  # Display the last 3 images\n",
    "            visualize_prediction(\n",
    "                images[i],\n",
    "                masks[i],\n",
    "                predictions[i]\n",
    "            )\n",
    "    \n",
    "    model.train()  # Set back to training mode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
