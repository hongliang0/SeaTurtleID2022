{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "resize_transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "annotation_file = \"./turtles-data/data/updated_annotations.json\"\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "class SeaTurtleDataset( Dataset ):\n",
    "    def __init__ (self, image_ids, transform = None):\n",
    "        self.coco = COCO('./turtles-data/data/updated_annotations.json')\n",
    "        self.image_ids = image_ids\n",
    "        self.cat_ids = self.coco.getCatIds()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__( self ):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image_data = self.coco.loadImgs([image_id])[0]\n",
    "\n",
    "        image_path = os.path.join('./turtles-data/data', image_data['file_name'])\n",
    "        image = cv.imread(image_path)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = self._getmask(self.image_ids[index], image)\n",
    "\n",
    "\n",
    "\n",
    "        if self.transform is  not  None : \n",
    "            augmentations = self.transform(image=image, mask =mask) \n",
    "            image = augmentations[ 'image' ] \n",
    "            mask = augmentations[ 'mask' ] \n",
    "\n",
    "        return image, mask \n",
    "    \n",
    "    def _getmask(self, image_id, image):\n",
    "        '''\n",
    "        Background (0):   000000000000000\n",
    "        Turtle Body (1):  111111100000000\n",
    "        Flippers (2):     112222200000000\n",
    "        Head (3):         112222233300000\n",
    "\n",
    "        Final Mask:       112222233300000\n",
    "        '''\n",
    "        categories = {\n",
    "            'turtle': 1,\n",
    "            'flipper': 2,\n",
    "            'head': 3\n",
    "        }\n",
    "        # Initialize the final mask with zeros\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        # Process each category\n",
    "        for category_name, category_id in categories.items():\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id, catIds=category_id, iscrowd=None)\n",
    "            annotations = self.coco.loadAnns(ann_ids)\n",
    "            \n",
    "            # Create a temporary mask for the current category\n",
    "            temp_mask = np.zeros_like(mask)\n",
    "            \n",
    "            for ann in annotations:\n",
    "                temp_mask += self.coco.annToMask(ann)\n",
    "            # Assign category-specific value to the final mask\n",
    "            if category_name == 'turtle':\n",
    "                mask[temp_mask > 0] = 1\n",
    "            elif category_name == 'flipper':\n",
    "                mask[temp_mask > 0] = 2\n",
    "            elif category_name == 'head':\n",
    "                mask[temp_mask > 0] = 3\n",
    "\n",
    "        return mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(coco.getImgIds()[:4000], test_size=0.1, random_state=42)\n",
    "train_ids, val_ids = train_test_split(train_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = SeaTurtleDataset(train_ids, transform=resize_transform)\n",
    "val_dataset = SeaTurtleDataset(val_ids, transform=resize_transform)\n",
    "test_dataset = SeaTurtleDataset(test_ids, transform=resize_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IoU(outputs, masks, target_class):\n",
    "    # Get the predicted class for each pixel\n",
    "    outputs = outputs.argmax(dim=1)\n",
    "    intersection = ((outputs == target_class) & (masks == target_class)).sum().item()\n",
    "    union = ((outputs == target_class) | (masks == target_class)).sum().item()\n",
    "    if union == 0:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return intersection / union\n",
    "    \n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=\"resnet101\",\n",
    "    encoder_weights=\"imagenet\",  \n",
    "    classes=4,                    \n",
    "    activation='softmax2d'\n",
    ")\n",
    "\n",
    "num_classes = 4  # background + 3 classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt generated function\n",
    "def visualize_prediction(image, mask, prediction, figsize=(15,5)):\n",
    "   # Convert tensors to numpy arrays\n",
    "   if isinstance(image, torch.Tensor):\n",
    "       image = image.cpu().numpy()\n",
    "       if image.shape[0] == 3:\n",
    "           image = image.transpose(1, 2, 0)\n",
    "       \n",
    "   if isinstance(mask, torch.Tensor):\n",
    "       mask = mask.cpu().numpy()\n",
    "       \n",
    "   if isinstance(prediction, torch.Tensor):\n",
    "       prediction = prediction.cpu().numpy()\n",
    "   \n",
    "   # Denormalize image\n",
    "   mean = np.array([0.485, 0.456, 0.406])\n",
    "   std = np.array([0.229, 0.224, 0.225])\n",
    "   image = np.clip((image * std + mean), 0, 1)\n",
    "   \n",
    "   # Create color maps for mask and prediction\n",
    "   colors = [(0,0,0), (1,0,0), (0,1,0), (0,0,1)]\n",
    "   colored_mask = np.zeros((*mask.shape, 3))\n",
    "   colored_pred = np.zeros((*prediction.shape, 3))\n",
    "   \n",
    "   for i, color in enumerate(colors):\n",
    "       colored_mask[mask == i] = color\n",
    "       colored_pred[prediction == i] = color\n",
    "   \n",
    "   # Plot\n",
    "   plt.figure(figsize=figsize)\n",
    "   \n",
    "   plt.subplot(1, 3, 1)\n",
    "   plt.imshow(image)\n",
    "   plt.title('Original Image')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.subplot(1, 3, 2)\n",
    "   plt.imshow(colored_mask)\n",
    "   plt.title('Ground Truth Mask')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.subplot(1, 3, 3)\n",
    "   plt.imshow(colored_pred)\n",
    "   plt.title('Model Prediction')\n",
    "   plt.axis('off')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Define class labels for each category\n",
    "turtle_class = 1\n",
    "flipper_class = 2\n",
    "head_class = 3\n",
    "\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Print every 5 batches\n",
    "        if batch_idx % 5 == 0:  \n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Average epoch loss\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images, masks = images.to(device), masks.to(device).long()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss after Epoch {epoch+1}: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Compute mIoU on the test set\n",
    "    turtle_IoUs, flipper_IoUs, head_IoUs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device).long()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute IoU for each category\n",
    "            for i in range(len(images)):  # Process each image in the batch\n",
    "                turtle_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], turtle_class))\n",
    "                flipper_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], flipper_class))\n",
    "                head_IoUs.append(compute_IoU(outputs[i:i+1], masks[i:i+1], head_class))\n",
    "\n",
    "    turtle_mIoU = np.nanmean(turtle_IoUs)\n",
    "    flipper_mIoU = np.nanmean(flipper_IoUs)\n",
    "    head_mIoU = np.nanmean(head_IoUs)\n",
    "    \n",
    "    print(f\"Turtle (Carapace) mIoU on Test Set after Epoch {epoch+1}: {turtle_mIoU:.4f}\")\n",
    "    print(f\"Flippers mIoU on Test Set after Epoch {epoch+1}: {flipper_mIoU:.4f}\")\n",
    "    print(f\"Head mIoU on Test Set after Epoch {epoch+1}: {head_mIoU:.4f}\")\n",
    "    \n",
    "    # Visualize last few predictions after each epoch\n",
    "    with torch.no_grad():\n",
    "        # Get a single batch from the train_loader\n",
    "        batch_images, batch_masks = next(iter(train_loader))\n",
    "        \n",
    "        # Select the last 3 images in the batch\n",
    "        images = batch_images[-3:].to(device)\n",
    "        masks = batch_masks[-3:]\n",
    "        \n",
    "        # Generate model predictions\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Visualize predictions\n",
    "        print(f\"\\nPredictions after Epoch {epoch+1}:\")\n",
    "        for i in range(3):  # Display the last 3 images\n",
    "            visualize_prediction(\n",
    "                images[i],\n",
    "                masks[i],\n",
    "                predictions[i]\n",
    "            )\n",
    "    \n",
    "    model.train()  # Set back to training mode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
