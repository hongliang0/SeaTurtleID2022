{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMP9517 Group Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model 1 : KNN\n",
    "\n",
    "# Please run this file after dataVisualisation.ipynb. Afterwards, you can run any model in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from pycocotools.coco import COCO\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for resizing and normalizing\n",
    "resize_transform = A.Compose([\n",
    "    A.Resize(128, 128),  # Resize images to a consistent size\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization\n",
    "])\n",
    "\n",
    "class SeaTurtleDatasetForKNN(Dataset):\n",
    "    def __init__(self, image_ids, transform=None):\n",
    "        self.coco = COCO(\"./turtles-data/data/updated_annotations.json\")\n",
    "        self.image_ids = image_ids\n",
    "        self.cat_ids = self.coco.getCatIds()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image\n",
    "        image_id = self.image_ids[index]\n",
    "        image_data = self.coco.loadImgs([image_id])[0]\n",
    "\n",
    "        image_path = os.path.join(\"./turtles-data/data\", image_data[\"file_name\"])\n",
    "        image = cv.imread(image_path)\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        # Prepare mask\n",
    "        mask = self._getmask(image_id, image)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = cv.resize(mask, (128, 128), interpolation=cv.INTER_NEAREST)\n",
    "\n",
    "        # Flatten images and masks for KNN input\n",
    "        return image.flatten(), mask.flatten()\n",
    "\n",
    "    def _getmask(self, image_id, image):\n",
    "        \"\"\"\n",
    "        Generate mask with labels for each category.\n",
    "        \"\"\"\n",
    "        categories = {\"turtle\": 1, \"flipper\": 2, \"head\": 3}\n",
    "        # Initialize the final mask with zeros\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "\n",
    "        # Process each category\n",
    "        for category_name, category_id in categories.items():\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id, catIds=category_id, iscrowd=None)\n",
    "            annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Create a temporary mask for the current category\n",
    "            temp_mask = np.zeros_like(mask)\n",
    "            for ann in annotations:\n",
    "                temp_mask += self.coco.annToMask(ann)\n",
    "            \n",
    "            # Assign category-specific value to the final mask\n",
    "            if category_name == \"turtle\":\n",
    "                mask[temp_mask > 0] = 1\n",
    "            elif category_name == \"flipper\":\n",
    "                mask[temp_mask > 0] = 2\n",
    "            elif category_name == \"head\":\n",
    "                mask[temp_mask > 0] = 3\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and map image IDs\n",
    "metadata_path = \"./turtles-data/data/metadata_splits.csv\"\n",
    "metadata = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=11.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the COCO object once\n",
    "coco = SeaTurtleDatasetForKNN([]).coco\n",
    "\n",
    "# Get all image IDs once\n",
    "image_ids = coco.getImgIds()\n",
    "\n",
    "# Create the img_to_filename dictionary\n",
    "img_to_filename = {img_id: coco.loadImgs(img_id)[0][\"file_name\"] for img_id in image_ids}\n",
    "\n",
    "# Create the reverse mapping file_to_img dictionary\n",
    "file_to_img = {v: k for k, v in img_to_filename.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image IDs for each split\n",
    "train_img_ids = [file_to_img[filename] for filename in metadata[metadata[\"split_open\"] == \"train\"][\"file_name\"] if filename in file_to_img]\n",
    "val_img_ids = [file_to_img[filename] for filename in metadata[metadata[\"split_open\"] == \"valid\"][\"file_name\"] if filename in file_to_img]\n",
    "test_img_ids = [file_to_img[filename] for filename in metadata[metadata[\"split_open\"] == \"test\"][\"file_name\"] if filename in file_to_img]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=71.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=74.31s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Create limited Subset datasets with only 200 samples for training and 100 for testing\n",
    "train_dataset = Subset(SeaTurtleDatasetForKNN(train_img_ids, transform=resize_transform), range(20))\n",
    "test_dataset = Subset(SeaTurtleDatasetForKNN(test_img_ids, transform=resize_transform), range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 4\n",
    "num_workers = 0\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "\tImage 0: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 0: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 0: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 0: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 1: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 1: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 1: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 1: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 2: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 2: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 2: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 2: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 3: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 3: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 3: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 3: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 4: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 4: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 4: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 4: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "Finished processing training data.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for KNN by processing in batches\n",
    "X_train, y_train = [], []\n",
    "\n",
    "print(\"Processing training data...\")\n",
    "\n",
    "for idx, (images, masks) in enumerate(train_loader):\n",
    "    for img, mask in zip(images, masks):\n",
    "        img_np = img.numpy().reshape(-1, 3)  # Flatten image to (16384, 3)\n",
    "        mask_np = mask.numpy().flatten()     # Flatten mask to (16384,)\n",
    "\n",
    "        # Check for size mismatch\n",
    "        if img_np.shape[0] != mask_np.shape[0]:\n",
    "            print(f\"Skipping image {idx} due to size mismatch.\")\n",
    "            print(f\"\\tImage {idx}: img_np: {img_np.shape}, mask_np: {mask_np.shape}\")\n",
    "            continue\n",
    "\n",
    "        # Append the entire flattened arrays\n",
    "        X_train.append(img_np)\n",
    "        y_train.append(mask_np)\n",
    "\n",
    "print(\"Finished processing training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing data...\n",
      "\tImage 0: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 0: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 0: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 0: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 1: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 1: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 1: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 1: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 2: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "\tImage 2: img_np: torch.Size([16384, 3]), mask_np: torch.Size([16384])\n",
      "Finished processing testing data.\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test, y_test = [], []\n",
    "\n",
    "print(\"Processing testing data...\")\n",
    "\n",
    "# Iterate over DataLoader batches\n",
    "for idx, (images, masks) in enumerate(test_loader):\n",
    "    for img, mask in zip(images, masks):\n",
    "        img_np = img.reshape(-1, 3)  # Flatten image to (N, 3)\n",
    "        mask_np = mask.flatten()     # Flatten mask to (N,)\n",
    "        print(f\"\\tImage {idx}: img_np: {img_np.shape}, mask_np: {mask_np.shape}\")\n",
    "\n",
    "        if img_np.shape[0] != mask_np.shape[0]:\n",
    "            print(f\"Skipping image {idx} due to size mismatch.\")\n",
    "            print(f\"\\tImage {idx}: img_np: {img_np.shape}, mask_np: {mask_np.shape}\")\n",
    "            continue\n",
    "\n",
    "        X_test.extend(img_np)\n",
    "        y_test.extend(mask_np)\n",
    "\n",
    "print(\"Finished processing testing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting to numpy arrays: zero-dimensional arrays cannot be concatenated\n",
      "X_train sample shapes: [(3,), (3,), (3,), (3,), (3,)]\n",
      "y_train sample shapes: [torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train sample shapes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mX_train[:\u001b[38;5;241m5\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train sample shapes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[y\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39my\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39my_train[:\u001b[38;5;241m5\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Print shapes for debugging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y_train shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[99], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(X_train)  \u001b[38;5;66;03m# Shape should be (num_samples * 16384, 3)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# Shape should be (num_samples * 16384,)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError converting to numpy arrays: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays\n",
    "try:\n",
    "    X_train = np.vstack(X_train)  # Shape should be (num_samples * 16384, 3)\n",
    "    y_train = np.concatenate(y_train).astype(int)  # Shape should be (num_samples * 16384,)\n",
    "except ValueError as e:\n",
    "    print(f\"Error converting to numpy arrays: {e}\")\n",
    "    print(f\"X_train sample shapes: {[x.shape for x in X_train[:5]]}\")\n",
    "    print(f\"y_train sample shapes: {[y.shape for y in y_train[:5]]}\")\n",
    "    raise e\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize and train KNN\u001b[39;00m\n\u001b[0;32m      2\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\61402\\anaconda3\\envs\\cv\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\61402\\anaconda3\\envs\\cv\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:233\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# KNeighborsClassifier.metric is not validated yet\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    214\u001b[0m )\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m        The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\61402\\anaconda3\\envs\\cv\\lib\\site-packages\\sklearn\\neighbors\\_base.py:456\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 456\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;66;03m# Classification targets require a specific format\u001b[39;00m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\61402\\anaconda3\\envs\\cv\\lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\61402\\anaconda3\\envs\\cv\\lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\61402\\anaconda3\\envs\\cv\\lib\\site-packages\\sklearn\\utils\\validation.py:938\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    939\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    940\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    942\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    943\u001b[0m         )\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    949\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Initialize and train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def display_images_with_segmentation(image_ids, knn_model):\n",
    "    for image_id in image_ids:\n",
    "        # Load image metadata\n",
    "        try:\n",
    "            img_info = coco.loadImgs(image_id)[0]\n",
    "            image_path = f\"turtles-data/data/{img_info['file_name']}\"\n",
    "        except TypeError:\n",
    "            print(f\"[DEBUG]: This image ID is missing: {image_id}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load the original image\n",
    "            image = np.array(Image.open(image_path))\n",
    "            plt.figure(figsize=(20, 10))\n",
    "\n",
    "            # Original Image\n",
    "            plt.subplot(1, 4, 1)\n",
    "            plt.imshow(image)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Original Image\")\n",
    "\n",
    "            # Ground Truth Annotations\n",
    "            plt.subplot(1, 4, 2)\n",
    "            plt.imshow(image)\n",
    "            cat_ids = coco.getCatIds()\n",
    "            ann_ids = coco.getAnnIds(imgIds=img_info[\"id\"], catIds=cat_ids, iscrowd=None)\n",
    "            anns = coco.loadAnns(ann_ids)\n",
    "            coco.showAnns(anns)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Ground Truth Annotations\")\n",
    "\n",
    "            # Ground Truth Mask\n",
    "            plt.subplot(1, 4, 3)\n",
    "            mask = np.zeros((img_info[\"height\"], img_info[\"width\"]), dtype=np.uint8)\n",
    "            for ann in anns:\n",
    "                mask += coco.annToMask(ann)\n",
    "            mask_resized = cv.resize(mask, (128, 128), interpolation=cv.INTER_NEAREST)\n",
    "            plt.imshow(mask_resized, cmap=\"plasma\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Ground Truth Mask\")\n",
    "\n",
    "            # KNN Predicted Mask\n",
    "            plt.subplot(1, 4, 4)\n",
    "            image_flattened = image.flatten().reshape(1, -1)\n",
    "            pred_mask_flat = knn_model.predict(image_flattened)\n",
    "            pred_mask = pred_mask_flat.reshape((128, 128))\n",
    "            plt.imshow(pred_mask, cmap=\"plasma\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"KNN Predicted Mask\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[DEBUG]: This image was already deleted: {image_id}. It has been removed from the dataset.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display segmentation results for a few test images\n",
    "sample_image_ids = test_img_ids[:10]  # Use a small sample for quick visualization\n",
    "display_images_with_segmentation(sample_image_ids, knn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
